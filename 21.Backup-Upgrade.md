# Kubernetes Cluster Upgrade with Kubeadm

This document provides a step-by-step guide for upgrading a Kubernetes cluster using **kubeadm** in a production environment, along with implementing etcd backup and restore to safeguard cluster data.

---

## Prerequisites
1. Ensure you have **kubeadm**, **kubectl**, and **kubelet** installed on all nodes.
2. Verify that the cluster is in a healthy state:
   ```bash
   kubectl get nodes
   kubectl get pods -A
   ```
3. Confirm the desired Kubernetes version:
   ```bash
   kubeadm upgrade plan
   ```
4. Backup your etcd data.

---

## Etcd Backup
### 1. Create an etcd Snapshot
Run the following command on the control plane node:
```bash
ETCDCTL_API=3 etcdctl snapshot save /path/to/etcd-backup.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key
```
### 2. Verify the Snapshot
Check the integrity of the backup:
```bash
ETCDCTL_API=3 etcdctl snapshot status /path/to/etcd-backup.db
```
### 3. Store Backup Securely
Upload the backup to a secure storage location (e.g., AWS S3, GCP Bucket).

---

## Kubernetes Version Upgrade Steps
### 1. Upgrade the Control Plane Node
#### Step 1: Upgrade kubeadm
```bash
sudo apt-get update && sudo apt-get install -y kubeadm=<desired-version>
```
#### Step 2: Run kubeadm Upgrade
```bash
sudo kubeadm upgrade apply <desired-version>
```
- Example:
  ```bash
  sudo kubeadm upgrade apply v1.27.1
  ```
#### Step 3: Upgrade kubelet and kubectl
```bash
sudo apt-get install -y kubelet=<desired-version> kubectl=<desired-version>
sudo systemctl restart kubelet
```

### 2. Upgrade Worker Nodes
#### Step 1: Drain the Node
```bash
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data
```
#### Step 2: Upgrade kubeadm
```bash
sudo apt-get update && sudo apt-get install -y kubeadm=<desired-version>
```
#### Step 3: Upgrade the Node
```bash
sudo kubeadm upgrade node
```
#### Step 4: Upgrade kubelet and kubectl
```bash
sudo apt-get install -y kubelet=<desired-version> kubectl=<desired-version>
sudo systemctl restart kubelet
```
#### Step 5: Uncordon the Node
```bash
kubectl uncordon <node-name>
```

### 3. Verify the Cluster Upgrade
- Check the cluster status:
  ```bash
  kubectl get nodes
  ```
- Verify component versions:
  ```bash
  kubectl version --short
  ```

---

## Etcd Restore
### 1. Stop Kubernetes Services
Stop the kube-apiserver service on the control plane node:
```bash
sudo systemctl stop kubelet
```

### 2. Restore the Snapshot
Restore the etcd snapshot:
```bash
ETCDCTL_API=3 etcdctl snapshot restore /path/to/etcd-backup.db \
  --data-dir /var/lib/etcd-backup \
  --initial-cluster <cluster-name>=https://127.0.0.1:2380 \
  --initial-cluster-token <unique-token> \
  --initial-advertise-peer-urls https://127.0.0.1:2380
```

### 3. Replace Etcd Data Directory
Move the restored data to the etcd data directory:
```bash
sudo mv /var/lib/etcd /var/lib/etcd-backup
```

### 4. Restart Kubernetes Services
Restart the kube-apiserver and other services:
```bash
sudo systemctl start kubelet
```

### 5. Verify Etcd Health
Check the etcd cluster health:
```bash
ETCDCTL_API=3 etcdctl endpoint health \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key
```

---

## Real-World Usage Example
### Scenario: Upgrading a Production Cluster
1. **Prepare**:
   - Notify stakeholders about potential downtime.
   - Backup etcd data.
2. **Upgrade**:
   - Upgrade control plane nodes one at a time.
   - Upgrade worker nodes incrementally.
3. **Post-Upgrade**:
   - Verify the cluster's functionality.
   - Restore etcd from the backup if issues arise.

By following these steps, you can ensure a smooth Kubernetes cluster upgrade with minimal risk to your production environment.
